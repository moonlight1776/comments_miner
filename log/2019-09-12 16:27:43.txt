12-Sep-19 16:27:44 - Model name 'bert/ernie1/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc). Assuming 'bert/ernie1/' is a path or url to a directory containing tokenizer files.
12-Sep-19 16:27:44 - Didn't find file bert/ernie1/added_tokens.json. We won't load it.
12-Sep-19 16:27:44 - Didn't find file bert/ernie1/special_tokens_map.json. We won't load it.
12-Sep-19 16:27:44 - Didn't find file bert/ernie1/tokenizer_config.json. We won't load it.
12-Sep-19 16:27:44 - loading file bert/ernie1/vocab.txt
12-Sep-19 16:27:44 - loading file None
12-Sep-19 16:27:44 - loading file None
12-Sep-19 16:27:44 - loading file None
12-Sep-19 16:27:44 - loading configuration file bert/ernie1/config.json
12-Sep-19 16:27:44 - Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "relu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 513,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 18000
}

12-Sep-19 16:27:44 - loading weights file bert/ernie1/pytorch_model.bin
12-Sep-19 16:29:15 - loading configuration file bert/ernie1/config.json
12-Sep-19 16:29:15 - Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "relu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 513,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 18000
}

12-Sep-19 16:29:15 - loading weights file bert/ernie1/pytorch_model.bin
12-Sep-19 16:30:40 - loading configuration file bert/ernie1/config.json
12-Sep-19 16:30:40 - Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "relu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 513,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 18000
}

12-Sep-19 16:30:40 - loading weights file bert/ernie1/pytorch_model.bin
